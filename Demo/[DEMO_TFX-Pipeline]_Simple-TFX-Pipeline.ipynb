{"cells":[{"cell_type":"markdown","metadata":{"id":"6x1ypzczQCwy"},"source":["# Tạo TFX Pipeline đơn giản"]},{"cell_type":"markdown","metadata":{"id":"mTMt_UM64wyY"},"source":["**Warning: Tập tin notebook này được run trên Google Colab**."]},{"cell_type":"markdown","metadata":{"id":"cblQ1OBF4wya"},"source":["TFX Pipeline đơn giản gồm các components: `CSVExampleGen`, `Trainer`, `Pusher` thực hiện trên tập dữ liệu Penguin."]},{"cell_type":"markdown","metadata":{"id":"Fmgi8ZvQkScg"},"source":["### Cài đặt TFX"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"as4OTe2ukSqm","outputId":"d87887a4-1745-4bcb-d00f-db78bdc5b05f","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1681142441123,"user_tz":-420,"elapsed":128932,"user":{"displayName":"Ngọc Tường Lê","userId":"05811417697192141904"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tfx\n","  Downloading tfx-1.12.0-py3-none-any.whl (2.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting packaging<21,>=20\n","  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-api-core<1.33\n","  Downloading google_api_core-1.32.0-py2.py3-none-any.whl (93 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.6/93.6 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.9/dist-packages (from tfx) (1.22.4)\n","Requirement already satisfied: typing-extensions<5,>=3.10.0.2 in /usr/local/lib/python3.9/dist-packages (from tfx) (4.5.0)\n","Collecting kubernetes<13,>=10.0.1\n","  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13 in /usr/local/lib/python3.9/dist-packages (from tfx) (3.20.3)\n","Collecting keras-tuner<2,>=1.0.4\n","  Downloading keras_tuner-1.3.4-py3-none-any.whl (172 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyarrow<7,>=6\n","  Downloading pyarrow-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.6/25.6 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-apitools<1,>=0.5\n","  Downloading google_apitools-0.5.32-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-data-validation<1.13.0,>=1.12.0\n","  Downloading tensorflow_data_validation-1.12.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.9/dist-packages (from tfx) (1.53.0)\n","Collecting tfx-bsl<1.13.0,>=1.12.0\n","  Downloading tfx_bsl-1.12.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (21.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ml-pipelines-sdk==1.12.0\n","  Downloading ml_pipelines_sdk-1.12.0-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting attrs<22,>=19.3.0\n","  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-aiplatform<1.18,>=1.6.2\n","  Downloading google_cloud_aiplatform-1.17.1-py2.py3-none-any.whl (2.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting click<8,>=7\n","  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker<5,>=4.1\n","  Downloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2<4,>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from tfx) (3.1.2)\n","Collecting google-api-python-client<2,>=1.8\n","  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-model-analysis<0.44.0,>=0.43.0\n","  Downloading tensorflow_model_analysis-0.43.0-py3-none-any.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting apache-beam[gcp]<3,>=2.40\n","  Downloading apache_beam-2.46.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-hub<0.13,>=0.9.0\n","  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-transform<1.13.0,>=1.12.0\n","  Downloading tensorflow_transform-1.12.0-py3-none-any.whl (439 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.8/439.8 KB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ml-metadata<1.13.0,>=1.12.0\n","  Downloading ml_metadata-1.12.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.9/dist-packages (from tfx) (1.3.9)\n","Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.9/dist-packages (from tfx) (1.4.0)\n","Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15\n","  Downloading tensorflow_serving_api-2.11.1-py2.py3-none-any.whl (37 kB)\n","Collecting google-cloud-bigquery<3,>=2.26.0\n","  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 KB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow<2.12,>=2.11.0\n","  Downloading tensorflow-2.11.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyyaml<6,>=3.12\n","  Downloading PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m630.1/630.1 KB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.4.2)\n","Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.9/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2022.7.1)\n","Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.9/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2022.10.31)\n","Collecting dill<0.3.2,>=0.3.1.1\n","  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.9/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.2.1)\n","Collecting fastavro<2,>=0.23.6\n","  Downloading fastavro-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pymongo<4.0.0,>=3.8.0\n","  Downloading pymongo-3.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (515 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.5/515.5 KB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting objsize<0.7.0,>=0.6.1\n","  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n","Collecting zstandard<1,>=0.18.0\n","  Downloading zstandard-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.8.2)\n","Requirement already satisfied: httplib2<0.22.0,>=0.8 in /usr/local/lib/python3.9/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.21.0)\n","Collecting fasteners<1.0,>=0.3\n","  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n","Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.22.2)\n","Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.27.1)\n","Collecting hdfs<3.0.0,>=2.1.0\n","  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n","Collecting crcmod<2.0,>=1.7\n","  Downloading crcmod-1.7.tar.gz (89 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting orjson<4.0\n","  Downloading orjson-3.8.10-cp39-cp39-manylinux_2_28_x86_64.whl (140 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-spanner<4,>=3.0.0\n","  Downloading google_cloud_spanner-3.30.0-py2.py3-none-any.whl (327 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.3/327.3 KB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-pubsub<3,>=2.1.0\n","  Downloading google_cloud_pubsub-2.16.0-py2.py3-none-any.whl (263 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.9/263.9 KB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-recommendations-ai<0.8.0,>=0.1.0\n","  Downloading google_cloud_recommendations_ai-0.7.1-py2.py3-none-any.whl (148 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-bigquery-storage<2.17,>=2.6.3\n","  Downloading google_cloud_bigquery_storage-2.16.2-py2.py3-none-any.whl (185 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.4/185.4 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0\n","  Downloading google_cloud_pubsublite-1.8.1-py2.py3-none-any.whl (288 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 KB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-language<2,>=1.3.0\n","  Downloading google_cloud_language-1.3.2-py2.py3-none-any.whl (83 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n","  Downloading google_cloud_bigtable-1.7.3-py2.py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.7/268.7 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-vision<4,>=2\n","  Downloading google_cloud_vision-3.4.1-py2.py3-none-any.whl (444 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.3/444.3 KB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0\n","  Downloading google_cloud_dlp-3.12.1-py2.py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-cloud-core<3,>=0.28.1 in /usr/local/lib/python3.9/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.3.2)\n","Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.1.0)\n","Collecting google-apitools<1,>=0.5\n","  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 KB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting google-cloud-datastore<2,>=1.8.0\n","  Downloading google_cloud_datastore-1.15.5-py2.py3-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.2/134.2 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cachetools<5,>=3.1.0\n","  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n","Collecting google-cloud-videointelligence<2,>=1.8.0\n","  Downloading google_cloud_videointelligence-1.16.3-py2.py3-none-any.whl (183 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.17.1)\n","Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from docker<5,>=4.1->tfx) (1.5.1)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker<5,>=4.1->tfx) (1.16.0)\n","Collecting google-auth<3,>=1.18.0\n","  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core<1.33->tfx) (1.59.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core<1.33->tfx) (67.6.1)\n","Collecting uritemplate<4dev,>=3.0.0\n","  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.9/dist-packages (from google-apitools<1,>=0.5->tfx) (4.1.3)\n","Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-aiplatform<1.18,>=1.6.2->tfx) (2.7.0)\n","Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-aiplatform<1.18,>=1.6.2->tfx) (2.11.0)\n","Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n","  Downloading google_cloud_resource_manager-1.9.1-py2.py3-none-any.whl (276 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 KB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-bigquery<3,>=2.26.0->tfx) (2.4.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2<4,>=2.7.3->tfx) (2.1.2)\n","Collecting kt-legacy\n","  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.9/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.26.15)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.9/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.3.1)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.9/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2022.12.7)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging<21,>=20->tfx) (3.0.9)\n","Collecting tensorflow-estimator<2.12,>=2.11.0\n","  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (1.14.1)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (1.6.3)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (3.8.0)\n","Collecting tensorboard<2.12,>=2.11\n","  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (0.4.0)\n","Collecting keras<2.12,>=2.11.0\n","  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (0.2.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (0.32.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (16.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (3.3.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (2.2.0)\n","Collecting protobuf<4,>=3.13\n","  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (23.3.3)\n","Collecting tensorflow-metadata<1.13,>=1.12.0\n","  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-data-validation<1.13.0,>=1.12.0->tfx) (1.4.4)\n","Collecting pyfarmhash<0.4,>=0.2\n","  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting joblib>=1.2.0\n","  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.9/dist-packages (from tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (7.7.1)\n","Requirement already satisfied: ipython<8,>=7 in /usr/local/lib/python3.9/dist-packages (from tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (7.34.0)\n","Requirement already satisfied: scipy<2,>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.10.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tfx) (0.40.0)\n","Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n","  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.6/115.6 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_api_core-2.10.1-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_api_core-2.10.0-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_api_core-2.9.0-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_api_core-2.8.1-py3-none-any.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_api_core-2.8.0-py3-none-any.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_api_core-1.33.2-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_api_core-1.33.1-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_api_core-1.33.0-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.40->tfx) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.40->tfx) (4.9)\n","Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n","  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n","Collecting google-cloud-dlp<4,>=3.0.0\n","  Downloading google_cloud_dlp-3.12.0-py2.py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_dlp-3.11.1-py2.py3-none-any.whl (131 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_dlp-3.11.0-py2.py3-none-any.whl (126 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_dlp-3.10.1-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_dlp-3.10.0-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_dlp-3.9.2-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.9/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.40->tfx) (1.48.2)\n","Collecting google-cloud-pubsub<3,>=2.1.0\n","  Downloading google_cloud_pubsub-2.15.2-py2.py3-none-any.whl (243 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.1/243.1 KB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_pubsub-2.15.1-py2.py3-none-any.whl (243 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.0/243.0 KB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_pubsub-2.15.0-py2.py3-none-any.whl (242 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.0/243.0 KB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_pubsub-2.14.1-py2.py3-none-any.whl (242 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 KB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_pubsub-2.14.0-py2.py3-none-any.whl (241 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.5/241.5 KB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_pubsub-2.13.12-py2.py3-none-any.whl (238 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.3/238.3 KB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_pubsub-2.13.11-py2.py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.0/237.0 KB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0\n","  Downloading google_cloud_pubsublite-1.8.0-py2.py3-none-any.whl (287 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 KB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_pubsublite-1.7.0-py2.py3-none-any.whl (273 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.9/273.9 KB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_pubsublite-1.6.0-py2.py3-none-any.whl (271 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.1/271.1 KB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting overrides<7.0.0,>=6.0.1\n","  Downloading overrides-6.5.0-py3-none-any.whl (17 kB)\n","Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n","  Downloading google_cloud_resource_manager-1.9.0-py2.py3-none-any.whl (276 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 KB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_resource_manager-1.8.1-py2.py3-none-any.whl (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.7/235.7 KB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_resource_manager-1.8.0-py2.py3-none-any.whl (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 KB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_resource_manager-1.7.0-py2.py3-none-any.whl (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 KB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.8/233.8 KB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-spanner<4,>=3.0.0\n","  Downloading google_cloud_spanner-3.29.0-py2.py3-none-any.whl (327 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.3/327.3 KB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_spanner-3.28.0-py2.py3-none-any.whl (327 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.1/327.1 KB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_spanner-3.27.1-py2.py3-none-any.whl (300 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 KB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_spanner-3.27.0-py2.py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 KB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_spanner-3.26.0-py2.py3-none-any.whl (294 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 KB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sqlparse>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.40->tfx) (0.4.3)\n","Collecting google-cloud-vision<4,>=2\n","  Downloading google_cloud_vision-3.4.0-py2.py3-none-any.whl (444 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.2/444.2 KB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_vision-3.3.1-py2.py3-none-any.whl (393 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.5/393.5 KB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_vision-3.3.0-py2.py3-none-any.whl (386 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.5/386.5 KB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_vision-3.2.0-py2.py3-none-any.whl (386 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.5/386.5 KB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading google_cloud_vision-3.1.4-py2.py3-none-any.whl (390 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.4/390.4 KB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.9/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=2.26.0->tfx) (1.5.0)\n","Collecting docopt\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.7.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (2.14.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.2.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (4.4.2)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (3.0.38)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.1.6)\n","Collecting jedi>=0.16\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (4.8.0)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (3.6.4)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (3.0.7)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.5.6)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx) (0.4.8)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tfx) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tfx) (2.0.12)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tfx) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tfx) (2.2.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tfx) (3.4.3)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib->kubernetes<13,>=10.0.1->tfx) (3.2.2)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (6.2)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (6.1.12)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.8.3)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tfx) (6.1.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.2.6)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (6.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tfx) (3.15.0)\n","Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.3.0)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.17.1)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.5.6)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.16.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (23.2.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (6.5.4)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.8.0)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.8.0)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (21.3.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (3.2.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (21.2.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (4.11.2)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (6.0.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.5.0)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.2.2)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.7.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (4.9.2)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.2.1)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.8.4)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (4.3.3)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (2.16.3)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.19.3)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.15.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (2.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.5.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (2.21)\n","Building wheels for collected packages: google-apitools, crcmod, dill, pyfarmhash, docopt\n","  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131038 sha256=f2d06f2028c5e516362b1a1cb9353ea987098a9037809e1853ef71cc09d57eef\n","  Stored in directory: /root/.cache/pip/wheels/6c/f8/60/b9e91899dbaf25b6314047d3daee379bdd8d61b1dc3fd5ec7f\n","  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for crcmod: filename=crcmod-1.7-cp39-cp39-linux_x86_64.whl size=36923 sha256=198b9ee4bc808076fb4e1e9d597bedb902e16fee6da094e58e9df743b2a493c9\n","  Stored in directory: /root/.cache/pip/wheels/4a/6c/a6/ffdd136310039bf226f2707a9a8e6857be7d70a3fc061f6b36\n","  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78545 sha256=61c8703a8d2f596184edd31f09aaf4a5c43525372b523977635b758573e93345\n","  Stored in directory: /root/.cache/pip/wheels/4f/0b/ce/75d96dd714b15e51cb66db631183ea3844e0c4a6d19741a149\n","  Building wheel for pyfarmhash (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp39-cp39-linux_x86_64.whl size=101507 sha256=8e17b53ed43401fe6cce6ae306c5226de5f84b2d48e6e897987f04ada9dcf9e6\n","  Stored in directory: /root/.cache/pip/wheels/de/2b/b1/c541160670d70f4b08c4786f4e155337d4baeaa3e01d9d1400\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=1dffaea94c2f142bf3aa61e181685b1fd145a794dffe3681f657327bcf88b89b\n","  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n","Successfully built google-apitools crcmod dill pyfarmhash docopt\n","Installing collected packages: pyfarmhash, kt-legacy, docopt, crcmod, zstandard, uritemplate, tensorflow-estimator, tensorboard-data-server, pyyaml, pymongo, pyarrow, protobuf, packaging, overrides, orjson, objsize, keras, joblib, jedi, fasteners, fastavro, dill, click, cachetools, attrs, tensorflow-hub, ml-metadata, hdfs, google-auth, docker, tensorflow-metadata, kubernetes, google-auth-oauthlib, google-apitools, google-api-core, apache-beam, tensorboard, grpc-google-iam-v1, google-api-python-client, tensorflow, ml-pipelines-sdk, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-resource-manager, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-bigquery-storage, google-cloud-bigquery, tensorflow-serving-api, keras-tuner, google-cloud-pubsublite, google-cloud-aiplatform, tfx-bsl, tensorflow-transform, tensorflow-model-analysis, tensorflow-data-validation, tfx\n","  Attempting uninstall: uritemplate\n","    Found existing installation: uritemplate 4.1.1\n","    Uninstalling uritemplate-4.1.1:\n","      Successfully uninstalled uritemplate-4.1.1\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.12.0\n","    Uninstalling tensorflow-estimator-2.12.0:\n","      Successfully uninstalled tensorflow-estimator-2.12.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.0\n","    Uninstalling tensorboard-data-server-0.7.0:\n","      Successfully uninstalled tensorboard-data-server-0.7.0\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 6.0\n","    Uninstalling PyYAML-6.0:\n","      Successfully uninstalled PyYAML-6.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 9.0.0\n","    Uninstalling pyarrow-9.0.0:\n","      Successfully uninstalled pyarrow-9.0.0\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 23.0\n","    Uninstalling packaging-23.0:\n","      Successfully uninstalled packaging-23.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.12.0\n","    Uninstalling keras-2.12.0:\n","      Successfully uninstalled keras-2.12.0\n","  Attempting uninstall: joblib\n","    Found existing installation: joblib 1.1.1\n","    Uninstalling joblib-1.1.1:\n","      Successfully uninstalled joblib-1.1.1\n","  Attempting uninstall: click\n","    Found existing installation: click 8.1.3\n","    Uninstalling click-8.1.3:\n","      Successfully uninstalled click-8.1.3\n","  Attempting uninstall: cachetools\n","    Found existing installation: cachetools 5.3.0\n","    Uninstalling cachetools-5.3.0:\n","      Successfully uninstalled cachetools-5.3.0\n","  Attempting uninstall: attrs\n","    Found existing installation: attrs 22.2.0\n","    Uninstalling attrs-22.2.0:\n","      Successfully uninstalled attrs-22.2.0\n","  Attempting uninstall: tensorflow-hub\n","    Found existing installation: tensorflow-hub 0.13.0\n","    Uninstalling tensorflow-hub-0.13.0:\n","      Successfully uninstalled tensorflow-hub-0.13.0\n","  Attempting uninstall: google-auth\n","    Found existing installation: google-auth 2.17.1\n","    Uninstalling google-auth-2.17.1:\n","      Successfully uninstalled google-auth-2.17.1\n","  Attempting uninstall: tensorflow-metadata\n","    Found existing installation: tensorflow-metadata 1.13.0\n","    Uninstalling tensorflow-metadata-1.13.0:\n","      Successfully uninstalled tensorflow-metadata-1.13.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: google-api-core\n","    Found existing installation: google-api-core 2.11.0\n","    Uninstalling google-api-core-2.11.0:\n","      Successfully uninstalled google-api-core-2.11.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.1\n","    Uninstalling tensorboard-2.12.1:\n","      Successfully uninstalled tensorboard-2.12.1\n","  Attempting uninstall: google-api-python-client\n","    Found existing installation: google-api-python-client 2.70.0\n","    Uninstalling google-api-python-client-2.70.0:\n","      Successfully uninstalled google-api-python-client-2.70.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.12.0\n","    Uninstalling tensorflow-2.12.0:\n","      Successfully uninstalled tensorflow-2.12.0\n","  Attempting uninstall: google-cloud-language\n","    Found existing installation: google-cloud-language 2.6.1\n","    Uninstalling google-cloud-language-2.6.1:\n","      Successfully uninstalled google-cloud-language-2.6.1\n","  Attempting uninstall: google-cloud-datastore\n","    Found existing installation: google-cloud-datastore 2.11.1\n","    Uninstalling google-cloud-datastore-2.11.1:\n","      Successfully uninstalled google-cloud-datastore-2.11.1\n","  Attempting uninstall: google-cloud-bigquery-storage\n","    Found existing installation: google-cloud-bigquery-storage 2.19.1\n","    Uninstalling google-cloud-bigquery-storage-2.19.1:\n","      Successfully uninstalled google-cloud-bigquery-storage-2.19.1\n","  Attempting uninstall: google-cloud-bigquery\n","    Found existing installation: google-cloud-bigquery 3.4.2\n","    Uninstalling google-cloud-bigquery-3.4.2:\n","      Successfully uninstalled google-cloud-bigquery-3.4.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray 2022.12.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n","statsmodels 0.13.5 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n","pandas-profiling 3.2.0 requires joblib~=1.1.0, but you have joblib 1.2.0 which is incompatible.\n","google-cloud-firestore 2.7.3 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.32.0 which is incompatible.\n","flask 2.2.3 requires click>=8.0, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed apache-beam-2.46.0 attrs-21.4.0 cachetools-4.2.4 click-7.1.2 crcmod-1.7 dill-0.3.1.1 docker-4.4.4 docopt-0.6.2 fastavro-1.7.3 fasteners-0.18 google-api-core-1.32.0 google-api-python-client-1.12.11 google-apitools-0.5.31 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-cloud-aiplatform-1.17.1 google-cloud-bigquery-2.34.4 google-cloud-bigquery-storage-2.16.2 google-cloud-bigtable-1.7.3 google-cloud-datastore-1.15.5 google-cloud-dlp-3.9.2 google-cloud-language-1.3.2 google-cloud-pubsub-2.13.11 google-cloud-pubsublite-1.6.0 google-cloud-recommendations-ai-0.7.1 google-cloud-resource-manager-1.6.3 google-cloud-spanner-3.26.0 google-cloud-videointelligence-1.16.3 google-cloud-vision-3.1.4 grpc-google-iam-v1-0.12.6 hdfs-2.7.0 jedi-0.18.2 joblib-1.2.0 keras-2.11.0 keras-tuner-1.3.4 kt-legacy-1.0.4 kubernetes-12.0.1 ml-metadata-1.12.0 ml-pipelines-sdk-1.12.0 objsize-0.6.1 orjson-3.8.10 overrides-6.5.0 packaging-20.9 protobuf-3.19.6 pyarrow-6.0.1 pyfarmhash-0.3.2 pymongo-3.13.0 pyyaml-5.4.1 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.11.1 tensorflow-data-validation-1.12.0 tensorflow-estimator-2.11.0 tensorflow-hub-0.12.0 tensorflow-metadata-1.12.0 tensorflow-model-analysis-0.43.0 tensorflow-serving-api-2.11.1 tensorflow-transform-1.12.0 tfx-1.12.0 tfx-bsl-1.12.0 uritemplate-3.0.1 zstandard-0.20.0\n","Found existing installation: shapely 2.0.1\n","Uninstalling shapely-2.0.1:\n","  Successfully uninstalled shapely-2.0.1\n"]}],"source":["try:\n","  import colab\n","  !pip install --upgrade pip\n","except:\n","  pass\n","\n","!pip install -U tfx\n","!pip uninstall shapely -y"]},{"cell_type":"markdown","metadata":{"id":"EwT0nov5QO1M"},"source":["### Các thư viện cần thiết\n","**Cần restart lại môi trường trước khi import thư viện.**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6jh7vKSRqPHb","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681142481165,"user_tz":-420,"elapsed":12103,"user":{"displayName":"Ngọc Tường Lê","userId":"05811417697192141904"}},"outputId":"8830f712-5730-47a3-85c1-a5ae7e59d7ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.11.1\n","TFX version: 1.12.0\n"]}],"source":["import tensorflow as tf\n","print('TensorFlow version: {}'.format(tf.__version__))\n","from tfx import v1 as tfx\n","print('TFX version: {}'.format(tfx.__version__))"]},{"cell_type":"markdown","metadata":{"id":"aDtLdSkvqPHe"},"source":["## 01. Setup các biến, collect tập dữ liệu:"]},{"cell_type":"markdown","metadata":{"id":"6vCAt6-G4wyj"},"source":["Setup các biến:\n","- Tên pipeline.\n","- Đường dẫn đến model.\n","- Đường dẫn đến metadata.\n","- Đường dẫn đến serving."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"EcUseqJaE2XN","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1681142536598,"user_tz":-420,"elapsed":14,"user":{"displayName":"Ngọc Tường Lê","userId":"05811417697192141904"}}},"outputs":[],"source":["import os\n","\n","PIPELINE_NAME = \"penguin-simple\"\n","\n","# Output directory to store artifacts generated from the pipeline.\n","PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n","# Path to a SQLite DB file to use as an MLMD storage.\n","METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n","# Output directory where created models from the pipeline will be exported.\n","SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n","\n","from absl import logging\n","logging.set_verbosity(logging.INFO)  # Set default logging level."]},{"cell_type":"markdown","metadata":{"id":"8F2SRwRLSYGa"},"source":["Download [Palmer Penguins dataset](https://allisonhorst.github.io/palmerpenguins/articles/intro.html) và lưu vào thư mục `tfx-data`.\n","\n","Tập dữ liệu có 4 thuộc tính đã được chuẩn hóa [0, 1]:\n","- culmen_length_mm.\n","- culmen_depth_mm.\n","- flipper_length_mm.\n","- body_mass_g.\n","\n","Tập dữ liệu có class là loài của chim cánh cụt, gồm: 0, 1, 2.\n","\n","Model dùng phân loại loài chim cánh cụt."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"4fxMs6u86acP","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681142536601,"user_tz":-420,"elapsed":14,"user":{"displayName":"Ngọc Tường Lê","userId":"05811417697192141904"}},"outputId":"178311b9-4784-4939-ba89-0fa2b52adc77"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/tmp/tfx-datatc8n_3vn/data.csv', <http.client.HTTPMessage at 0x7f4cd8dbf1c0>)"]},"metadata":{},"execution_count":3}],"source":["import urllib.request\n","import tempfile\n","\n","DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n","_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n","_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n","urllib.request.urlretrieve(_data_url, _data_filepath)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"-eSz28UDSnlG","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681142538168,"user_tz":-420,"elapsed":1577,"user":{"displayName":"Ngọc Tường Lê","userId":"05811417697192141904"}},"outputId":"a0d859ac-5d35-4a56-b985-7f6905ad024b"},"outputs":[{"output_type":"stream","name":"stdout","text":["species,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g\n","0,0.2545454545454545,0.6666666666666666,0.15254237288135594,0.2916666666666667\n","0,0.26909090909090905,0.5119047619047618,0.23728813559322035,0.3055555555555556\n","0,0.29818181818181805,0.5833333333333334,0.3898305084745763,0.1527777777777778\n","0,0.16727272727272732,0.7380952380952381,0.3559322033898305,0.20833333333333334\n","0,0.26181818181818167,0.892857142857143,0.3050847457627119,0.2638888888888889\n","0,0.24727272727272717,0.5595238095238096,0.15254237288135594,0.2569444444444444\n","0,0.25818181818181823,0.773809523809524,0.3898305084745763,0.5486111111111112\n","0,0.32727272727272727,0.5357142857142859,0.1694915254237288,0.1388888888888889\n","0,0.23636363636363636,0.9642857142857142,0.3220338983050847,0.3055555555555556\n"]}],"source":["!head {_data_filepath}"]},{"cell_type":"markdown","metadata":{"id":"nH6gizcpSwWV"},"source":["## 02. Tạo pipeline:"]},{"cell_type":"markdown","metadata":{"id":"B7RXCQtR4wyo"},"source":["Pipeline gồm:\n","- `CsvExampleGen`: đọc dữ liệu từ csv.\n","- `Trainer`: train model sử dụng `keras`.\n","- `Pusher`: triển khai model đã train.\n","\n","Viết thuật toán DNN cho phân loại sử dụng `keras`, thuật toán này sẽ được lưu trong file riêng biệt:\n","\n","Dùng `Generic Trainer`, ta sẽ tạo hàm `run_fn` hàm này sẽ được gọi bởi `Trainer`."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"aES7Hv5QTDK3","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1681142538170,"user_tz":-420,"elapsed":24,"user":{"displayName":"Ngọc Tường Lê","userId":"05811417697192141904"}}},"outputs":[],"source":["_trainer_module_file = 'penguin_trainer.py'"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Gnc67uQNTDfW","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681142538171,"user_tz":-420,"elapsed":23,"user":{"displayName":"Ngọc Tường Lê","userId":"05811417697192141904"}},"outputId":"defa058e-5f22-4d8b-d6be-7a18de62ea1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing penguin_trainer.py\n"]}],"source":["%%writefile {_trainer_module_file}\n","\n","from typing import List\n","from absl import logging\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow_transform.tf_metadata import schema_utils\n","\n","from tfx import v1 as tfx\n","from tfx_bsl.public import tfxio\n","from tensorflow_metadata.proto.v0 import schema_pb2\n","\n","_FEATURE_KEYS = [\n","    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n","]\n","_LABEL_KEY = 'species'\n","\n","_TRAIN_BATCH_SIZE = 20\n","_EVAL_BATCH_SIZE = 10\n","\n","# Since we're not generating or creating a schema, we will instead create\n","# a feature spec.  Since there are a fairly small number of features this is\n","# manageable for this dataset.\n","_FEATURE_SPEC = {\n","    **{\n","        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n","           for feature in _FEATURE_KEYS\n","       },\n","    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n","}\n","\n","\n","def _input_fn(file_pattern: List[str],\n","              data_accessor: tfx.components.DataAccessor,\n","              schema: schema_pb2.Schema,\n","              batch_size: int = 200) -> tf.data.Dataset:\n","  \"\"\"Generates features and label for training.\n","\n","  Args:\n","    file_pattern: List of paths or patterns of input tfrecord files.\n","    data_accessor: DataAccessor for converting input to RecordBatch.\n","    schema: schema of the input data.\n","    batch_size: representing the number of consecutive elements of returned\n","      dataset to combine in a single batch\n","\n","  Returns:\n","    A dataset that contains (features, indices) tuple where features is a\n","      dictionary of Tensors, and indices is a single Tensor of label indices.\n","  \"\"\"\n","  return data_accessor.tf_dataset_factory(\n","      file_pattern,\n","      tfxio.TensorFlowDatasetOptions(\n","          batch_size=batch_size, label_key=_LABEL_KEY),\n","      schema=schema).repeat()\n","\n","\n","def _build_keras_model() -> tf.keras.Model:\n","  \"\"\"Creates a DNN Keras model for classifying penguin data.\n","\n","  Returns:\n","    A Keras Model.\n","  \"\"\"\n","  # The model below is built with Functional API, please refer to\n","  # https://www.tensorflow.org/guide/keras/overview for all API options.\n","  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n","  d = keras.layers.concatenate(inputs)\n","  for _ in range(2):\n","    d = keras.layers.Dense(8, activation='relu')(d)\n","  outputs = keras.layers.Dense(3)(d)\n","\n","  model = keras.Model(inputs=inputs, outputs=outputs)\n","  model.compile(\n","      optimizer=keras.optimizers.Adam(1e-2),\n","      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n","\n","  model.summary(print_fn=logging.info)\n","  return model\n","\n","\n","# TFX Trainer will call this function.\n","def run_fn(fn_args: tfx.components.FnArgs):\n","  \"\"\"Train the model based on given args.\n","\n","  Args:\n","    fn_args: Holds args used to train the model as name/value pairs.\n","  \"\"\"\n","\n","  # This schema is usually either an output of SchemaGen or a manually-curated\n","  # version provided by pipeline author. A schema can also derived from TFT\n","  # graph if a Transform component is used. In the case when either is missing,\n","  # `schema_from_feature_spec` could be used to generate schema from very simple\n","  # feature_spec, but the schema returned would be very primitive.\n","  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n","\n","  train_dataset = _input_fn(\n","      fn_args.train_files,\n","      fn_args.data_accessor,\n","      schema,\n","      batch_size=_TRAIN_BATCH_SIZE)\n","  eval_dataset = _input_fn(\n","      fn_args.eval_files,\n","      fn_args.data_accessor,\n","      schema,\n","      batch_size=_EVAL_BATCH_SIZE)\n","\n","  model = _build_keras_model()\n","  model.fit(\n","      train_dataset,\n","      steps_per_epoch=fn_args.train_steps,\n","      validation_data=eval_dataset,\n","      validation_steps=fn_args.eval_steps)\n","\n","  # The result of the training should be saved in `fn_args.serving_model_dir`\n","  # directory.\n","  model.save(fn_args.serving_model_dir, save_format='tf')"]},{"cell_type":"markdown","metadata":{"id":"w3OkNz3gTLwM"},"source":["Kế tiếp ta sẽ định nghĩa pipeline với 3 thành phần trên."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"M49yYVNBTPd4","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1681142538172,"user_tz":-420,"elapsed":18,"user":{"displayName":"Ngọc Tường Lê","userId":"05811417697192141904"}}},"outputs":[],"source":["def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n","                     module_file: str, serving_model_dir: str,\n","                     metadata_path: str) -> tfx.dsl.Pipeline:\n","  \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n","  # Brings data into the pipeline.\n","  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n","\n","  # Uses user-provided Python function that trains a model.\n","  trainer = tfx.components.Trainer(\n","      module_file=module_file,\n","      examples=example_gen.outputs['examples'],\n","      train_args=tfx.proto.TrainArgs(num_steps=100),\n","      eval_args=tfx.proto.EvalArgs(num_steps=5))\n","\n","  # Pushes the model to a filesystem destination.\n","  pusher = tfx.components.Pusher(\n","      model=trainer.outputs['model'],\n","      push_destination=tfx.proto.PushDestination(\n","          filesystem=tfx.proto.PushDestination.Filesystem(\n","              base_directory=serving_model_dir)))\n","\n","  # Following three components will be included in the pipeline.\n","  components = [\n","      example_gen,\n","      trainer,\n","      pusher,\n","  ]\n","\n","  return tfx.dsl.Pipeline(\n","      pipeline_name=pipeline_name,\n","      pipeline_root=pipeline_root,\n","      metadata_connection_config=tfx.orchestration.metadata\n","      .sqlite_metadata_connection_config(metadata_path),\n","      components=components)"]},{"cell_type":"markdown","metadata":{"id":"7mp0AkmrPdUb"},"source":["## 03. Run pipeline:"]},{"cell_type":"markdown","metadata":{"id":"7vT91Twx4wyu"},"source":["Sử dụng `LocalDagRunner` để run pipeline."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"fAtfOZTYWJu-","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1681142553140,"user_tz":-420,"elapsed":14985,"user":{"displayName":"Ngọc Tường Lê","userId":"05811417697192141904"}},"outputId":"08d913b2-99ce-4253-880d-e3606bbf014a"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:absl:Generating ephemeral wheel package for '/content/penguin_trainer.py' (including modules: ['penguin_trainer']).\n","INFO:absl:User module package has hash fingerprint version a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.\n","INFO:absl:Executing: ['/usr/bin/python3', '/tmp/tmpnns4cvqr/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmp8lac3h48', '--dist-dir', '/tmp/tmpa9u8bx6d']\n","INFO:absl:Successfully built user code wheel distribution at 'pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl'; target user module is 'penguin_trainer'.\n","INFO:absl:Full user module path is 'penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl'\n","INFO:absl:Using deployment config:\n"," executor_specs {\n","  key: \"CsvExampleGen\"\n","  value {\n","    beam_executable_spec {\n","      python_executor_spec {\n","        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n","      }\n","    }\n","  }\n","}\n","executor_specs {\n","  key: \"Pusher\"\n","  value {\n","    python_class_executable_spec {\n","      class_path: \"tfx.components.pusher.executor.Executor\"\n","    }\n","  }\n","}\n","executor_specs {\n","  key: \"Trainer\"\n","  value {\n","    python_class_executable_spec {\n","      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n","    }\n","  }\n","}\n","custom_driver_specs {\n","  key: \"CsvExampleGen\"\n","  value {\n","    python_class_executable_spec {\n","      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n","    }\n","  }\n","}\n","metadata_connection_config {\n","  database_connection_config {\n","    sqlite {\n","      filename_uri: \"metadata/penguin-simple/metadata.db\"\n","      connection_mode: READWRITE_OPENCREATE\n","    }\n","  }\n","}\n","\n","INFO:absl:Using connection config:\n"," sqlite {\n","  filename_uri: \"metadata/penguin-simple/metadata.db\"\n","  connection_mode: READWRITE_OPENCREATE\n","}\n","\n","INFO:absl:Component CsvExampleGen is running.\n","INFO:absl:Running launcher for node_info {\n","  type {\n","    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n","  }\n","  id: \"CsvExampleGen\"\n","}\n","contexts {\n","  contexts {\n","    type {\n","      name: \"pipeline\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"penguin-simple\"\n","      }\n","    }\n","  }\n","  contexts {\n","    type {\n","      name: \"pipeline_run\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"2023-04-10T16:02:18.712790\"\n","      }\n","    }\n","  }\n","  contexts {\n","    type {\n","      name: \"node\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"penguin-simple.CsvExampleGen\"\n","      }\n","    }\n","  }\n","}\n","outputs {\n","  outputs {\n","    key: \"examples\"\n","    value {\n","      artifact_spec {\n","        type {\n","          name: \"Examples\"\n","          properties {\n","            key: \"span\"\n","            value: INT\n","          }\n","          properties {\n","            key: \"split_names\"\n","            value: STRING\n","          }\n","          properties {\n","            key: \"version\"\n","            value: INT\n","          }\n","          base_type: DATASET\n","        }\n","      }\n","    }\n","  }\n","}\n","parameters {\n","  parameters {\n","    key: \"input_base\"\n","    value {\n","      field_value {\n","        string_value: \"/tmp/tfx-datatc8n_3vn\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"input_config\"\n","    value {\n","      field_value {\n","        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"output_config\"\n","    value {\n","      field_value {\n","        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"output_data_format\"\n","    value {\n","      field_value {\n","        int_value: 6\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"output_file_format\"\n","    value {\n","      field_value {\n","        int_value: 5\n","      }\n","    }\n","  }\n","}\n","downstream_nodes: \"Trainer\"\n","execution_options {\n","  caching_options {\n","  }\n","}\n","\n","INFO:absl:MetadataStore with DB connection initialized\n","INFO:absl:[CsvExampleGen] Resolved inputs: ({},)\n","INFO:absl:select span and version = (0, None)\n","INFO:absl:latest span and version = (0, None)\n","INFO:absl:MetadataStore with DB connection initialized\n","INFO:absl:Going to run a new execution 1\n","INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=1, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-simple/CsvExampleGen/examples/1\"\n","custom_properties {\n","  key: \"input_fingerprint\"\n","  value {\n","    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1681142536,sum_checksum:1681142536\"\n","  }\n","}\n","custom_properties {\n","  key: \"span\"\n","  value {\n","    int_value: 0\n","  }\n","}\n",", artifact_type: name: \"Examples\"\n","properties {\n","  key: \"span\"\n","  value: INT\n","}\n","properties {\n","  key: \"split_names\"\n","  value: STRING\n","}\n","properties {\n","  key: \"version\"\n","  value: INT\n","}\n","base_type: DATASET\n",")]}), exec_properties={'output_file_format': 5, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'output_data_format': 6, 'input_base': '/tmp/tfx-datatc8n_3vn', 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:25648,xor_checksum:1681142536,sum_checksum:1681142536'}, execution_output_uri='pipelines/penguin-simple/CsvExampleGen/.system/executor_execution/1/executor_output.pb', stateful_working_dir='pipelines/penguin-simple/CsvExampleGen/.system/stateful_working_dir/2023-04-10T16:02:18.712790', tmp_dir='pipelines/penguin-simple/CsvExampleGen/.system/executor_execution/1/.temp/', pipeline_node=node_info {\n","  type {\n","    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n","  }\n","  id: \"CsvExampleGen\"\n","}\n","contexts {\n","  contexts {\n","    type {\n","      name: \"pipeline\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"penguin-simple\"\n","      }\n","    }\n","  }\n","  contexts {\n","    type {\n","      name: \"pipeline_run\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"2023-04-10T16:02:18.712790\"\n","      }\n","    }\n","  }\n","  contexts {\n","    type {\n","      name: \"node\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"penguin-simple.CsvExampleGen\"\n","      }\n","    }\n","  }\n","}\n","outputs {\n","  outputs {\n","    key: \"examples\"\n","    value {\n","      artifact_spec {\n","        type {\n","          name: \"Examples\"\n","          properties {\n","            key: \"span\"\n","            value: INT\n","          }\n","          properties {\n","            key: \"split_names\"\n","            value: STRING\n","          }\n","          properties {\n","            key: \"version\"\n","            value: INT\n","          }\n","          base_type: DATASET\n","        }\n","      }\n","    }\n","  }\n","}\n","parameters {\n","  parameters {\n","    key: \"input_base\"\n","    value {\n","      field_value {\n","        string_value: \"/tmp/tfx-datatc8n_3vn\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"input_config\"\n","    value {\n","      field_value {\n","        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"output_config\"\n","    value {\n","      field_value {\n","        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"output_data_format\"\n","    value {\n","      field_value {\n","        int_value: 6\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"output_file_format\"\n","    value {\n","      field_value {\n","        int_value: 5\n","      }\n","    }\n","  }\n","}\n","downstream_nodes: \"Trainer\"\n","execution_options {\n","  caching_options {\n","  }\n","}\n",", pipeline_info=id: \"penguin-simple\"\n",", pipeline_run_id='2023-04-10T16:02:18.712790')\n","INFO:absl:Generating examples.\n","WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"]},{"output_type":"display_data","data":{"application/javascript":["\n","        if (typeof window.interactive_beam_jquery == 'undefined') {\n","          var jqueryScript = document.createElement('script');\n","          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n","          jqueryScript.type = 'text/javascript';\n","          jqueryScript.onload = function() {\n","            var datatableScript = document.createElement('script');\n","            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n","            datatableScript.type = 'text/javascript';\n","            datatableScript.onload = function() {\n","              window.interactive_beam_jquery = jQuery.noConflict(true);\n","              window.interactive_beam_jquery(document).ready(function($){\n","                \n","              });\n","            }\n","            document.head.appendChild(datatableScript);\n","          };\n","          document.head.appendChild(jqueryScript);\n","        } else {\n","          window.interactive_beam_jquery(document).ready(function($){\n","            \n","          });\n","        }"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:absl:Processing input csv data /tmp/tfx-datatc8n_3vn/* to TFExample.\n","WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n","INFO:absl:Examples generated.\n","INFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n","INFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n","INFO:absl:Cleaning up stateless execution info.\n","INFO:absl:Execution 1 succeeded.\n","INFO:absl:Cleaning up stateful execution info.\n","INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-simple/CsvExampleGen/examples/1\"\n","custom_properties {\n","  key: \"input_fingerprint\"\n","  value {\n","    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1681142536,sum_checksum:1681142536\"\n","  }\n","}\n","custom_properties {\n","  key: \"span\"\n","  value {\n","    int_value: 0\n","  }\n","}\n",", artifact_type: name: \"Examples\"\n","properties {\n","  key: \"span\"\n","  value: INT\n","}\n","properties {\n","  key: \"split_names\"\n","  value: STRING\n","}\n","properties {\n","  key: \"version\"\n","  value: INT\n","}\n","base_type: DATASET\n",")]}) for execution 1\n","INFO:absl:MetadataStore with DB connection initialized\n","INFO:absl:Component CsvExampleGen is finished.\n","INFO:absl:Component Trainer is running.\n","INFO:absl:Running launcher for node_info {\n","  type {\n","    name: \"tfx.components.trainer.component.Trainer\"\n","    base_type: TRAIN\n","  }\n","  id: \"Trainer\"\n","}\n","contexts {\n","  contexts {\n","    type {\n","      name: \"pipeline\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"penguin-simple\"\n","      }\n","    }\n","  }\n","  contexts {\n","    type {\n","      name: \"pipeline_run\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"2023-04-10T16:02:18.712790\"\n","      }\n","    }\n","  }\n","  contexts {\n","    type {\n","      name: \"node\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"penguin-simple.Trainer\"\n","      }\n","    }\n","  }\n","}\n","inputs {\n","  inputs {\n","    key: \"examples\"\n","    value {\n","      channels {\n","        producer_node_query {\n","          id: \"CsvExampleGen\"\n","        }\n","        context_queries {\n","          type {\n","            name: \"pipeline\"\n","          }\n","          name {\n","            field_value {\n","              string_value: \"penguin-simple\"\n","            }\n","          }\n","        }\n","        context_queries {\n","          type {\n","            name: \"pipeline_run\"\n","          }\n","          name {\n","            field_value {\n","              string_value: \"2023-04-10T16:02:18.712790\"\n","            }\n","          }\n","        }\n","        context_queries {\n","          type {\n","            name: \"node\"\n","          }\n","          name {\n","            field_value {\n","              string_value: \"penguin-simple.CsvExampleGen\"\n","            }\n","          }\n","        }\n","        artifact_query {\n","          type {\n","            name: \"Examples\"\n","            base_type: DATASET\n","          }\n","        }\n","        output_key: \"examples\"\n","      }\n","      min_count: 1\n","    }\n","  }\n","}\n","outputs {\n","  outputs {\n","    key: \"model\"\n","    value {\n","      artifact_spec {\n","        type {\n","          name: \"Model\"\n","          base_type: MODEL\n","        }\n","      }\n","    }\n","  }\n","  outputs {\n","    key: \"model_run\"\n","    value {\n","      artifact_spec {\n","        type {\n","          name: \"ModelRun\"\n","        }\n","      }\n","    }\n","  }\n","}\n","parameters {\n","  parameters {\n","    key: \"custom_config\"\n","    value {\n","      field_value {\n","        string_value: \"null\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"eval_args\"\n","    value {\n","      field_value {\n","        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"module_path\"\n","    value {\n","      field_value {\n","        string_value: \"penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"train_args\"\n","    value {\n","      field_value {\n","        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n","      }\n","    }\n","  }\n","}\n","upstream_nodes: \"CsvExampleGen\"\n","downstream_nodes: \"Pusher\"\n","execution_options {\n","  caching_options {\n","  }\n","}\n","\n","INFO:absl:MetadataStore with DB connection initialized\n","WARNING:absl:ArtifactQuery.property_predicate is not supported.\n","INFO:absl:[Trainer] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\n","type_id: 15\n","uri: \"pipelines/penguin-simple/CsvExampleGen/examples/1\"\n","properties {\n","  key: \"split_names\"\n","  value {\n","    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n","  }\n","}\n","custom_properties {\n","  key: \"file_format\"\n","  value {\n","    string_value: \"tfrecords_gzip\"\n","  }\n","}\n","custom_properties {\n","  key: \"input_fingerprint\"\n","  value {\n","    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1681142536,sum_checksum:1681142536\"\n","  }\n","}\n","custom_properties {\n","  key: \"is_external\"\n","  value {\n","    int_value: 0\n","  }\n","}\n","custom_properties {\n","  key: \"payload_format\"\n","  value {\n","    string_value: \"FORMAT_TF_EXAMPLE\"\n","  }\n","}\n","custom_properties {\n","  key: \"span\"\n","  value {\n","    int_value: 0\n","  }\n","}\n","custom_properties {\n","  key: \"state\"\n","  value {\n","    string_value: \"published\"\n","  }\n","}\n","custom_properties {\n","  key: \"tfx_version\"\n","  value {\n","    string_value: \"1.12.0\"\n","  }\n","}\n","state: LIVE\n","create_time_since_epoch: 1681142542901\n","last_update_time_since_epoch: 1681142542901\n",", artifact_type: id: 15\n","name: \"Examples\"\n","properties {\n","  key: \"span\"\n","  value: INT\n","}\n","properties {\n","  key: \"split_names\"\n","  value: STRING\n","}\n","properties {\n","  key: \"version\"\n","  value: INT\n","}\n","base_type: DATASET\n",")]},)\n","INFO:absl:MetadataStore with DB connection initialized\n","INFO:absl:Going to run a new execution 2\n","INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=2, input_dict={'examples': [Artifact(artifact: id: 1\n","type_id: 15\n","uri: \"pipelines/penguin-simple/CsvExampleGen/examples/1\"\n","properties {\n","  key: \"split_names\"\n","  value {\n","    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n","  }\n","}\n","custom_properties {\n","  key: \"file_format\"\n","  value {\n","    string_value: \"tfrecords_gzip\"\n","  }\n","}\n","custom_properties {\n","  key: \"input_fingerprint\"\n","  value {\n","    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1681142536,sum_checksum:1681142536\"\n","  }\n","}\n","custom_properties {\n","  key: \"is_external\"\n","  value {\n","    int_value: 0\n","  }\n","}\n","custom_properties {\n","  key: \"payload_format\"\n","  value {\n","    string_value: \"FORMAT_TF_EXAMPLE\"\n","  }\n","}\n","custom_properties {\n","  key: \"span\"\n","  value {\n","    int_value: 0\n","  }\n","}\n","custom_properties {\n","  key: \"state\"\n","  value {\n","    string_value: \"published\"\n","  }\n","}\n","custom_properties {\n","  key: \"tfx_version\"\n","  value {\n","    string_value: \"1.12.0\"\n","  }\n","}\n","state: LIVE\n","create_time_since_epoch: 1681142542901\n","last_update_time_since_epoch: 1681142542901\n",", artifact_type: id: 15\n","name: \"Examples\"\n","properties {\n","  key: \"span\"\n","  value: INT\n","}\n","properties {\n","  key: \"split_names\"\n","  value: STRING\n","}\n","properties {\n","  key: \"version\"\n","  value: INT\n","}\n","base_type: DATASET\n",")]}, output_dict=defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"pipelines/penguin-simple/Trainer/model/2\"\n",", artifact_type: name: \"Model\"\n","base_type: MODEL\n",")], 'model_run': [Artifact(artifact: uri: \"pipelines/penguin-simple/Trainer/model_run/2\"\n",", artifact_type: name: \"ModelRun\"\n",")]}), exec_properties={'eval_args': '{\\n  \"num_steps\": 5\\n}', 'module_path': 'penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl', 'custom_config': 'null', 'train_args': '{\\n  \"num_steps\": 100\\n}'}, execution_output_uri='pipelines/penguin-simple/Trainer/.system/executor_execution/2/executor_output.pb', stateful_working_dir='pipelines/penguin-simple/Trainer/.system/stateful_working_dir/2023-04-10T16:02:18.712790', tmp_dir='pipelines/penguin-simple/Trainer/.system/executor_execution/2/.temp/', pipeline_node=node_info {\n","  type {\n","    name: \"tfx.components.trainer.component.Trainer\"\n","    base_type: TRAIN\n","  }\n","  id: \"Trainer\"\n","}\n","contexts {\n","  contexts {\n","    type {\n","      name: \"pipeline\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"penguin-simple\"\n","      }\n","    }\n","  }\n","  contexts {\n","    type {\n","      name: \"pipeline_run\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"2023-04-10T16:02:18.712790\"\n","      }\n","    }\n","  }\n","  contexts {\n","    type {\n","      name: \"node\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"penguin-simple.Trainer\"\n","      }\n","    }\n","  }\n","}\n","inputs {\n","  inputs {\n","    key: \"examples\"\n","    value {\n","      channels {\n","        producer_node_query {\n","          id: \"CsvExampleGen\"\n","        }\n","        context_queries {\n","          type {\n","            name: \"pipeline\"\n","          }\n","          name {\n","            field_value {\n","              string_value: \"penguin-simple\"\n","            }\n","          }\n","        }\n","        context_queries {\n","          type {\n","            name: \"pipeline_run\"\n","          }\n","          name {\n","            field_value {\n","              string_value: \"2023-04-10T16:02:18.712790\"\n","            }\n","          }\n","        }\n","        context_queries {\n","          type {\n","            name: \"node\"\n","          }\n","          name {\n","            field_value {\n","              string_value: \"penguin-simple.CsvExampleGen\"\n","            }\n","          }\n","        }\n","        artifact_query {\n","          type {\n","            name: \"Examples\"\n","            base_type: DATASET\n","          }\n","        }\n","        output_key: \"examples\"\n","      }\n","      min_count: 1\n","    }\n","  }\n","}\n","outputs {\n","  outputs {\n","    key: \"model\"\n","    value {\n","      artifact_spec {\n","        type {\n","          name: \"Model\"\n","          base_type: MODEL\n","        }\n","      }\n","    }\n","  }\n","  outputs {\n","    key: \"model_run\"\n","    value {\n","      artifact_spec {\n","        type {\n","          name: \"ModelRun\"\n","        }\n","      }\n","    }\n","  }\n","}\n","parameters {\n","  parameters {\n","    key: \"custom_config\"\n","    value {\n","      field_value {\n","        string_value: \"null\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"eval_args\"\n","    value {\n","      field_value {\n","        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"module_path\"\n","    value {\n","      field_value {\n","        string_value: \"penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"train_args\"\n","    value {\n","      field_value {\n","        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n","      }\n","    }\n","  }\n","}\n","upstream_nodes: \"CsvExampleGen\"\n","downstream_nodes: \"Pusher\"\n","execution_options {\n","  caching_options {\n","  }\n","}\n",", pipeline_info=id: \"penguin-simple\"\n",", pipeline_run_id='2023-04-10T16:02:18.712790')\n","INFO:absl:Train on the 'train' split when train_args.splits is not set.\n","INFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\n","INFO:absl:udf_utils.get_fn {'eval_args': '{\\n  \"num_steps\": 5\\n}', 'module_path': 'penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl', 'custom_config': 'null', 'train_args': '{\\n  \"num_steps\": 100\\n}'} 'run_fn'\n","INFO:absl:Installing 'pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl' to a temporary directory.\n","INFO:absl:Executing: ['/usr/bin/python3', '-m', 'pip', 'install', '--target', '/tmp/tmpwepyjht9', 'pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl']\n","INFO:absl:Successfully installed 'pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl'.\n","INFO:absl:Training model.\n","INFO:absl:Feature body_mass_g has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature culmen_depth_mm has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature culmen_length_mm has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature flipper_length_mm has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature species has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n","Instructions for updating:\n","Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n","INFO:absl:Feature body_mass_g has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature culmen_depth_mm has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature culmen_length_mm has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature flipper_length_mm has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature species has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature body_mass_g has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature culmen_depth_mm has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature culmen_length_mm has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature flipper_length_mm has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature species has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature body_mass_g has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature culmen_depth_mm has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature culmen_length_mm has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature flipper_length_mm has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Feature species has a shape dim {\n","  size: 1\n","}\n",". Setting to DenseTensor.\n","INFO:absl:Model: \"model\"\n","INFO:absl:__________________________________________________________________________________________________\n","INFO:absl: Layer (type)                   Output Shape         Param #     Connected to                     \n","INFO:absl:==================================================================================================\n","INFO:absl: culmen_length_mm (InputLayer)  [(None, 1)]          0           []                               \n","INFO:absl:                                                                                                  \n","INFO:absl: culmen_depth_mm (InputLayer)   [(None, 1)]          0           []                               \n","INFO:absl:                                                                                                  \n","INFO:absl: flipper_length_mm (InputLayer)  [(None, 1)]         0           []                               \n","INFO:absl:                                                                                                  \n","INFO:absl: body_mass_g (InputLayer)       [(None, 1)]          0           []                               \n","INFO:absl:                                                                                                  \n","INFO:absl: concatenate (Concatenate)      (None, 4)            0           ['culmen_length_mm[0][0]',       \n","INFO:absl:                                                                  'culmen_depth_mm[0][0]',        \n","INFO:absl:                                                                  'flipper_length_mm[0][0]',      \n","INFO:absl:                                                                  'body_mass_g[0][0]']            \n","INFO:absl:                                                                                                  \n","INFO:absl: dense (Dense)                  (None, 8)            40          ['concatenate[0][0]']            \n","INFO:absl:                                                                                                  \n","INFO:absl: dense_1 (Dense)                (None, 8)            72          ['dense[0][0]']                  \n","INFO:absl:                                                                                                  \n","INFO:absl: dense_2 (Dense)                (None, 3)            27          ['dense_1[0][0]']                \n","INFO:absl:                                                                                                  \n","INFO:absl:==================================================================================================\n","INFO:absl:Total params: 139\n","INFO:absl:Trainable params: 139\n","INFO:absl:Non-trainable params: 0\n","INFO:absl:__________________________________________________________________________________________________\n"]},{"output_type":"stream","name":"stdout","text":["100/100 [==============================] - 2s 7ms/step - loss: 0.6230 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.2853 - val_sparse_categorical_accuracy: 0.9000\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n","INFO:absl:Training complete. Model written to pipelines/penguin-simple/Trainer/model/2/Format-Serving. ModelRun written to pipelines/penguin-simple/Trainer/model_run/2\n","INFO:absl:Cleaning up stateless execution info.\n","INFO:absl:Execution 2 succeeded.\n","INFO:absl:Cleaning up stateful execution info.\n","INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"pipelines/penguin-simple/Trainer/model/2\"\n",", artifact_type: name: \"Model\"\n","base_type: MODEL\n",")], 'model_run': [Artifact(artifact: uri: \"pipelines/penguin-simple/Trainer/model_run/2\"\n",", artifact_type: name: \"ModelRun\"\n",")]}) for execution 2\n","INFO:absl:MetadataStore with DB connection initialized\n","INFO:absl:Component Trainer is finished.\n","INFO:absl:Component Pusher is running.\n","INFO:absl:Running launcher for node_info {\n","  type {\n","    name: \"tfx.components.pusher.component.Pusher\"\n","    base_type: DEPLOY\n","  }\n","  id: \"Pusher\"\n","}\n","contexts {\n","  contexts {\n","    type {\n","      name: \"pipeline\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"penguin-simple\"\n","      }\n","    }\n","  }\n","  contexts {\n","    type {\n","      name: \"pipeline_run\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"2023-04-10T16:02:18.712790\"\n","      }\n","    }\n","  }\n","  contexts {\n","    type {\n","      name: \"node\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"penguin-simple.Pusher\"\n","      }\n","    }\n","  }\n","}\n","inputs {\n","  inputs {\n","    key: \"model\"\n","    value {\n","      channels {\n","        producer_node_query {\n","          id: \"Trainer\"\n","        }\n","        context_queries {\n","          type {\n","            name: \"pipeline\"\n","          }\n","          name {\n","            field_value {\n","              string_value: \"penguin-simple\"\n","            }\n","          }\n","        }\n","        context_queries {\n","          type {\n","            name: \"pipeline_run\"\n","          }\n","          name {\n","            field_value {\n","              string_value: \"2023-04-10T16:02:18.712790\"\n","            }\n","          }\n","        }\n","        context_queries {\n","          type {\n","            name: \"node\"\n","          }\n","          name {\n","            field_value {\n","              string_value: \"penguin-simple.Trainer\"\n","            }\n","          }\n","        }\n","        artifact_query {\n","          type {\n","            name: \"Model\"\n","            base_type: MODEL\n","          }\n","        }\n","        output_key: \"model\"\n","      }\n","    }\n","  }\n","}\n","outputs {\n","  outputs {\n","    key: \"pushed_model\"\n","    value {\n","      artifact_spec {\n","        type {\n","          name: \"PushedModel\"\n","          base_type: MODEL\n","        }\n","      }\n","    }\n","  }\n","}\n","parameters {\n","  parameters {\n","    key: \"custom_config\"\n","    value {\n","      field_value {\n","        string_value: \"null\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"push_destination\"\n","    value {\n","      field_value {\n","        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-simple\\\"\\n  }\\n}\"\n","      }\n","    }\n","  }\n","}\n","upstream_nodes: \"Trainer\"\n","execution_options {\n","  caching_options {\n","  }\n","}\n","\n","INFO:absl:MetadataStore with DB connection initialized\n","WARNING:absl:ArtifactQuery.property_predicate is not supported.\n","INFO:absl:[Pusher] Resolved inputs: ({'model': [Artifact(artifact: id: 2\n","type_id: 17\n","uri: \"pipelines/penguin-simple/Trainer/model/2\"\n","custom_properties {\n","  key: \"is_external\"\n","  value {\n","    int_value: 0\n","  }\n","}\n","custom_properties {\n","  key: \"state\"\n","  value {\n","    string_value: \"published\"\n","  }\n","}\n","custom_properties {\n","  key: \"tfx_version\"\n","  value {\n","    string_value: \"1.12.0\"\n","  }\n","}\n","state: LIVE\n","create_time_since_epoch: 1681142551792\n","last_update_time_since_epoch: 1681142551792\n",", artifact_type: id: 17\n","name: \"Model\"\n","base_type: MODEL\n",")]},)\n","INFO:absl:MetadataStore with DB connection initialized\n","INFO:absl:Going to run a new execution 3\n","INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=3, input_dict={'model': [Artifact(artifact: id: 2\n","type_id: 17\n","uri: \"pipelines/penguin-simple/Trainer/model/2\"\n","custom_properties {\n","  key: \"is_external\"\n","  value {\n","    int_value: 0\n","  }\n","}\n","custom_properties {\n","  key: \"state\"\n","  value {\n","    string_value: \"published\"\n","  }\n","}\n","custom_properties {\n","  key: \"tfx_version\"\n","  value {\n","    string_value: \"1.12.0\"\n","  }\n","}\n","state: LIVE\n","create_time_since_epoch: 1681142551792\n","last_update_time_since_epoch: 1681142551792\n",", artifact_type: id: 17\n","name: \"Model\"\n","base_type: MODEL\n",")]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-simple/Pusher/pushed_model/3\"\n",", artifact_type: name: \"PushedModel\"\n","base_type: MODEL\n",")]}), exec_properties={'custom_config': 'null', 'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"serving_model/penguin-simple\"\\n  }\\n}'}, execution_output_uri='pipelines/penguin-simple/Pusher/.system/executor_execution/3/executor_output.pb', stateful_working_dir='pipelines/penguin-simple/Pusher/.system/stateful_working_dir/2023-04-10T16:02:18.712790', tmp_dir='pipelines/penguin-simple/Pusher/.system/executor_execution/3/.temp/', pipeline_node=node_info {\n","  type {\n","    name: \"tfx.components.pusher.component.Pusher\"\n","    base_type: DEPLOY\n","  }\n","  id: \"Pusher\"\n","}\n","contexts {\n","  contexts {\n","    type {\n","      name: \"pipeline\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"penguin-simple\"\n","      }\n","    }\n","  }\n","  contexts {\n","    type {\n","      name: \"pipeline_run\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"2023-04-10T16:02:18.712790\"\n","      }\n","    }\n","  }\n","  contexts {\n","    type {\n","      name: \"node\"\n","    }\n","    name {\n","      field_value {\n","        string_value: \"penguin-simple.Pusher\"\n","      }\n","    }\n","  }\n","}\n","inputs {\n","  inputs {\n","    key: \"model\"\n","    value {\n","      channels {\n","        producer_node_query {\n","          id: \"Trainer\"\n","        }\n","        context_queries {\n","          type {\n","            name: \"pipeline\"\n","          }\n","          name {\n","            field_value {\n","              string_value: \"penguin-simple\"\n","            }\n","          }\n","        }\n","        context_queries {\n","          type {\n","            name: \"pipeline_run\"\n","          }\n","          name {\n","            field_value {\n","              string_value: \"2023-04-10T16:02:18.712790\"\n","            }\n","          }\n","        }\n","        context_queries {\n","          type {\n","            name: \"node\"\n","          }\n","          name {\n","            field_value {\n","              string_value: \"penguin-simple.Trainer\"\n","            }\n","          }\n","        }\n","        artifact_query {\n","          type {\n","            name: \"Model\"\n","            base_type: MODEL\n","          }\n","        }\n","        output_key: \"model\"\n","      }\n","    }\n","  }\n","}\n","outputs {\n","  outputs {\n","    key: \"pushed_model\"\n","    value {\n","      artifact_spec {\n","        type {\n","          name: \"PushedModel\"\n","          base_type: MODEL\n","        }\n","      }\n","    }\n","  }\n","}\n","parameters {\n","  parameters {\n","    key: \"custom_config\"\n","    value {\n","      field_value {\n","        string_value: \"null\"\n","      }\n","    }\n","  }\n","  parameters {\n","    key: \"push_destination\"\n","    value {\n","      field_value {\n","        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-simple\\\"\\n  }\\n}\"\n","      }\n","    }\n","  }\n","}\n","upstream_nodes: \"Trainer\"\n","execution_options {\n","  caching_options {\n","  }\n","}\n",", pipeline_info=id: \"penguin-simple\"\n",", pipeline_run_id='2023-04-10T16:02:18.712790')\n","WARNING:absl:Pusher is going to push the model without validation. Consider using Evaluator or InfraValidator in your pipeline.\n","INFO:absl:Model version: 1681142551\n","INFO:absl:Model written to serving path serving_model/penguin-simple/1681142551.\n","INFO:absl:Model pushed to pipelines/penguin-simple/Pusher/pushed_model/3.\n","INFO:absl:Cleaning up stateless execution info.\n","INFO:absl:Execution 3 succeeded.\n","INFO:absl:Cleaning up stateful execution info.\n","INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-simple/Pusher/pushed_model/3\"\n",", artifact_type: name: \"PushedModel\"\n","base_type: MODEL\n",")]}) for execution 3\n","INFO:absl:MetadataStore with DB connection initialized\n","INFO:absl:Component Pusher is finished.\n"]}],"source":["tfx.orchestration.LocalDagRunner().run(\n","  _create_pipeline(\n","      pipeline_name=PIPELINE_NAME,\n","      pipeline_root=PIPELINE_ROOT,\n","      data_root=DATA_ROOT,\n","      module_file=_trainer_module_file,\n","      serving_model_dir=SERVING_MODEL_DIR,\n","      metadata_path=METADATA_PATH))"]},{"cell_type":"markdown","metadata":{"id":"ppERq0Mj6xvW"},"source":["Nếu thấy component cuối cùng thành công, nghĩa là pipeline đã run thành công.\n","\n","Sau đó ta sẽ thấy model đã train sẽ được lưu trong thư mục serving đã định nghĩa."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"NTHROkqX6yHx","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681142553141,"user_tz":-420,"elapsed":21,"user":{"displayName":"Ngọc Tường Lê","userId":"05811417697192141904"}},"outputId":"06487d5e-40d2-48e6-904c-c99d886bb279"},"outputs":[{"output_type":"stream","name":"stdout","text":["serving_model/penguin-simple\n","serving_model/penguin-simple/1681142551\n","serving_model/penguin-simple/1681142551/fingerprint.pb\n","serving_model/penguin-simple/1681142551/assets\n","serving_model/penguin-simple/1681142551/variables\n","serving_model/penguin-simple/1681142551/variables/variables.index\n","serving_model/penguin-simple/1681142551/variables/variables.data-00000-of-00001\n","serving_model/penguin-simple/1681142551/saved_model.pb\n","serving_model/penguin-simple/1681142551/keras_metadata.pb\n"]}],"source":["# List files in created model directory.\n","!find {SERVING_MODEL_DIR}"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/tensorflow/tfx/blob/master/docs/tutorials/tfx/penguin_simple.ipynb","timestamp":1681139241748}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}